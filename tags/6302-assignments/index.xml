















<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    
    <title>6302 assignments · Welcome to Venkiverse</title>
    
    <link>https://venkiphy6.github.io/tags/6302-assignments/</link>
    <description>Recent content in 6302 assignments · Welcome to Venkiverse</description>
    
    <language>en-us</language>
    
    
    
    <copyright>Made with Hugo</copyright>
    
    
    <lastBuildDate>Fri, 09 Dec 2022 17:32:24 -0600</lastBuildDate>
    
    
    <atom:link href="https://venkiphy6.github.io/tags/6302-assignments/index.xml" rel="self" type="application/rss+xml"/>
    
    
    <item>
      
      <title>Project: Network Structure of the Digital Advertising Marketplace</title>
      
      
      <link>https://venkiphy6.github.io/writings/03_admapper/</link>
      <guid>https://venkiphy6.github.io/writings/03_admapper/</guid>
      
      
      <pubDate>Fri, 09 Dec 2022 17:32:24 -0600</pubDate>
      
      
      
      <description><![CDATA[<p>I coordinated a project with my classmates (Chun-Yen Pan, McKayla Sharp, Sarah Stukalin and Xiaoyan Zhang) for a course on &ldquo;Methods of data collection and production&rsquo;&rsquo;(<a href="https://catalog.utdallas.edu/2021/graduate/courses/epps6302">EPPS 6302</a>). The project culminated in a report which can be read <a href="https://github.com/VenkiPhy6/admapper/blob/main/Admapper_Project_Report.pdf">here</a>. The Github repository containing the data collected in the project and the code used for the data collection and analysis can be found here: <a href="https://github.com/VenkiPhy6/admapper">admapper Github repo</a></p>
<p>This project involved collating a sample frame of URLs of US news sources followed by scraping data (<code>ads.txt</code>) from these URLs. Then we proceeded to make a graph database on <a href="https://neo4j.com/">Neo4j</a> for scaling up the dataset in the future and finally performed bipartite network analysis using the &lsquo;Method of Reflections&rsquo; technique. A major focus of this class project was directed towards collection, cleaning and creation of the database since the class was on data collection. We also explored the theoretical aspects behind the digital advertising market and explicated it in the report. I am thankful to all my classmates for being patient in working with me on this nuanced subject!</p>
<p>If you have read my other posts on this blog, you will notice that this project is an extension of my interest in regulation of digital platforms which was kindled somewhere between late 2020 and early 2021, thanks to <a href="https://school.takshashila.org.in/gcpp-technology-policy">the Tech Policy course</a> at the Takshashila Institution. Earlier, in May 2021, I had coordinated a project with the cohort of <a href="https://www.complexityweekend.com/">Complexity Weekend 2021</a> (<a href="https://github.com/BAFurtado">Bernado Furtado</a>, Kishor Acharya among others in the cohort). You can check out the <a href="https://github.com/BAFurtado/albatross">Github repository</a> of that project containing some skeleton code and a presentation that was done in the closing livestream of the Complexity Weekend event <a href="https://www.youtube.com/watch?v=lQWGflw2qcc&amp;list=PL-gMn-lI3siVTB15wkk3XHGce1uM7lOVU&amp;t=5728s">here</a>. That project had a different aim and attempted to create an Agent-Based Model (ABM) to explain digital platforms. It succeeded in creating a very basic version of the ABM. But, this is the first time I have taken a jab at the problem using real world data. Alongside this project, I was also learning Economic Complexity (which I have written about <a href="https://venkiverse.notion.site/An-Introduction-to-Economic-Complexity-70ca4e0b15e44a138cc7701ebf9f2703">here</a>) for the work I was doing at the <a href="https://artscilab.utdallas.edu/">ArtSciLab</a> and ended up stumbling upon the &lsquo;Method of Reflections&rsquo; technique for analyzing bipartite networks which I used in this project. Overall, this project pulled from several past experiences and learnings. I am fairly certain this project will serve as a stepping stone to more work on digital platform regulations in the future!</p>
]]></description>
      
    </item>
    
    <item>
      
      <title>A simple analysis of a Twitter dataset</title>
      
      
      <link>https://venkiphy6.github.io/writings/02_twitter_analysis/</link>
      <guid>https://venkiphy6.github.io/writings/02_twitter_analysis/</guid>
      
      
      <pubDate>Mon, 07 Nov 2022 17:38:24 -0600</pubDate>
      
      
      
      <description><![CDATA[<p>I was given an assignment in class to analyze some Twitter data. Here is the prompt from the assignment:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-tex" data-lang="tex"><span style="display:flex;"><span>1. Use your developer account to extract Joe Biden’s tweets in last three months
</span></span><span style="display:flex;"><span>2. Analyze the tweet data
</span></span><span style="display:flex;"><span>   1. Most likes
</span></span><span style="display:flex;"><span>   2. Most retweets
</span></span><span style="display:flex;"><span>   3. Most replied
</span></span><span style="display:flex;"><span>   4. What are the hashtags in above messages 
</span></span><span style="display:flex;"><span>3. Collect a bag of hashtags for the following topics
</span></span><span style="display:flex;"><span>   1. Black Lives Matter
</span></span><span style="display:flex;"><span>   2. PLA Taiwan
</span></span><span style="display:flex;"><span>   3. COVID vaccines
</span></span></code></pre></div><p>Below is what I did for this assignment. I started by installing a bunch of packages I needed. Then I authenticated the Twitter developer account I had created.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-R" data-lang="R"><span style="display:flex;"><span><span style="color:#008000">#Installing packages</span>
</span></span><span style="display:flex;"><span>ptbu=c(<span style="color:#a31515">&#34;rtweet&#34;</span>, <span style="color:#a31515">&#34;dplyr&#34;</span>, <span style="color:#a31515">&#34;tidyverse&#34;</span>,<span style="color:#a31515">&#34;tidytext&#34;</span>,  <span style="color:#a31515">&#34;data.table&#34;</span>, <span style="color:#a31515">&#34;maps&#34;</span>, <span style="color:#a31515">&#34;syuzhet&#34;</span>,<span style="color:#a31515">&#34;lubridate&#34;</span>, <span style="color:#a31515">&#34;tm&#34;</span>)
</span></span><span style="display:flex;"><span>install.packages(ptbu)
</span></span><span style="display:flex;"><span>lapply(ptbu, require, character.only = <span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000">#Simple authentication!</span>
</span></span><span style="display:flex;"><span>auth_setup_default()
</span></span></code></pre></div><p>After that I extracted all of Joe Biden&rsquo;s tweets from 1 Aug 2022 till the date I had pulled the data (Nov 1, 2022). So three months of Joe Biden&rsquo;s tweets as was mentioned in the prompt 2.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-R" data-lang="R"><span style="display:flex;"><span><span style="color:#008000">#Getting Biden&#39;s timeline (i.e what he has tweeted)</span>
</span></span><span style="display:flex;"><span>biden_tweets &lt;- get_timeline(<span style="color:#a31515">&#34;939091&#34;</span>, n = 500)
</span></span><span style="display:flex;"><span><span style="color:#008000">#Filtering to last 3 months&#39; tweets alone</span>
</span></span><span style="display:flex;"><span>biden_3tweets &lt;- biden_tweets %&gt;% filter(created_at &gt; <span style="color:#a31515">&#39;2022-08-01&#39;</span>) <span style="color:#008000">#Getting last 3 months tweets</span>
</span></span></code></pre></div><p>Then I sorted the tweets by <code>favorite_count</code> in descending order</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-R" data-lang="R"><span style="display:flex;"><span><span style="color:#008000">#Most favorited</span>
</span></span><span style="display:flex;"><span>biden_3tweets_favs &lt;- biden_3tweets[order(biden_3tweets$favorite_count, decreasing=T),][c(<span style="color:#a31515">&#34;text&#34;</span>, <span style="color:#a31515">&#34;created_at&#34;</span>, <span style="color:#a31515">&#34;favorite_count&#34;</span>)]
</span></span><span style="display:flex;"><span>biden_3tweets_favs[1, <span style="color:#a31515">&#34;id_str&#34;</span>][[1]]
</span></span></code></pre></div><p>From this the most favorited tweet in my dataset turned out to be this one:</p>

    <style type="text/css">
      .twitter-tweet {
        font: 14px/1.45 -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;
        border-left: 4px solid #2b7bb9;
        padding-left: 1.5em;
        color: #555;
      }
      .twitter-tweet a {
        color: #2b7bb9;
        text-decoration: none;
      }
      blockquote.twitter-tweet a:hover,
      blockquote.twitter-tweet a:focus {
        text-decoration: underline;
      }
    </style>
  <blockquote class="twitter-tweet"><p lang="en" dir="ltr">Donald Trump and MAGA Republicans are a threat to the very soul of this country.</p>&mdash; Joe Biden (@JoeBiden) <a href="https://twitter.com/JoeBiden/status/1565492666120523778?ref_src=twsrc%5Etfw">September 2, 2022</a></blockquote>

<p>Then I sorted the tweets by <code>retweet_count</code> in descending order</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-R" data-lang="R"><span style="display:flex;"><span><span style="color:#008000">#Most retweets</span>
</span></span><span style="display:flex;"><span>biden_3tweets_rts &lt;- biden_3tweets[order(biden_3tweets$retweet_count, decreasing=T),][c(<span style="color:#a31515">&#34;text&#34;</span>, <span style="color:#a31515">&#34;created_at&#34;</span>, <span style="color:#a31515">&#34;retweet_count&#34;</span>)]
</span></span><span style="display:flex;"><span>biden_3tweets_rts[1, <span style="color:#a31515">&#34;id_str&#34;</span>][[1]]
</span></span></code></pre></div><p>From this the most retweeted tweet in my dataset turned out to be this one:</p>

  <blockquote class="twitter-tweet"><p lang="en" dir="ltr">As I’ve said before, no one should be in jail just for using or possessing marijuana.<br><br>Today, I’m taking steps to end our failed approach. Allow me to lay them out.</p>&mdash; President Biden (@POTUS) <a href="https://twitter.com/POTUS/status/1578097875480895489?ref_src=twsrc%5Etfw">October 6, 2022</a></blockquote>

<p>I tried to repeat this same exercise with <code>reply_count</code> but the dataset just had <code>NA</code> in the entire column. I am unsure why the Twitter API responded with nothing in this column. This is a good example of the unreliability of API methods - you are always at the mercy of whichever organizations controls that API!</p>
<p>Then I tried to obtain hashtags from these tweets.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-R" data-lang="R"><span style="display:flex;"><span>lapply(head(biden_3tweets_favs, 50)[<span style="color:#a31515">&#39;text&#39;</span>], function(elt) str_extract(elt, regex(<span style="color:#a31515">&#39;#[a-zA-Z0-9_]+&#39;</span>)))
</span></span></code></pre></div><p>To my surprise I got nothing. I checked Joe Biden&rsquo;s twitter timeline and saw that he never uses hashtags! I am not sure why this is the case. Anyways, with that prompt 2 is done. Moving on to prompt 3&hellip;</p>
<p>For prompt 1, I pulled the timeline of a specific user. Now I need to pull tweets based on the keywords provided.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-R" data-lang="R"><span style="display:flex;"><span><span style="color:#008000">#Pulling tweets</span>
</span></span><span style="display:flex;"><span>blm &lt;- rtweet::search_tweets(q = <span style="color:#a31515">&#34;Black Lives Matter&#34;</span>, n = 5000, lang = <span style="color:#a31515">&#34;en&#34;</span>, retryonratelimit = <span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>pla &lt;- rtweet::search_tweets(q = <span style="color:#a31515">&#34;PLA Taiwan&#34;</span>, n = 5000, lang = <span style="color:#a31515">&#34;en&#34;</span>, retryonratelimit = <span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>covid &lt;- rtweet::search_tweets(q = <span style="color:#a31515">&#34;COVID vaccines&#34;</span>, n = 5000, lang = <span style="color:#a31515">&#34;en&#34;</span>, retryonratelimit = <span style="color:#00f">TRUE</span>)
</span></span></code></pre></div><p>Now, to pull hashtags from these datasets, I created a function that uses regexp to extract anything that starts with a pound sign (or <a href="https://www.youtube.com/watch?v=HEVOM0VycMI">octothorpe</a> if you want to sound cool!). The function also cleans things up and returns a tidy vector of hashtags. I proceeded to call the function on my datasets.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-R" data-lang="R"><span style="display:flex;"><span><span style="color:#008000">#Bag of hashtags</span>
</span></span><span style="display:flex;"><span>hashtag_extractor &lt;- function (tweets) {
</span></span><span style="display:flex;"><span>  hashtags &lt;- lapply(tweets, function(tweet) str_extract_all(tweet, regex(<span style="color:#a31515">&#39;#[a-zA-Z0-9_]+&#39;</span>)))
</span></span><span style="display:flex;"><span>  hashtags &lt;- unlist(hashtags, use.names = F, recursive = <span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>  hashtags &lt;- hashtags[!is.na(hashtags)]
</span></span><span style="display:flex;"><span>  return(hashtags)
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>blm_hashtags &lt;- hashtag_extractor(blm[<span style="color:#a31515">&#39;text&#39;</span>])
</span></span><span style="display:flex;"><span>pla_hashtags &lt;- hashtag_extractor(pla[<span style="color:#a31515">&#39;text&#39;</span>])
</span></span><span style="display:flex;"><span>covid_hashtags &lt;- hashtag_extractor(covid[<span style="color:#a31515">&#39;text&#39;</span>])
</span></span></code></pre></div><p>Now I needed a way to visualize these hashtags. So, <a href="/writings/01_churchill/">like last time</a>, I proceeded to create word clouds.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-R" data-lang="R"><span style="display:flex;"><span><span style="color:#008000">#Word clouds of hashtags</span>
</span></span><span style="display:flex;"><span>library(easypackages)
</span></span><span style="display:flex;"><span>packages(<span style="color:#a31515">&#34;wordcloud&#34;</span>,<span style="color:#a31515">&#34;RColorBrewer&#34;</span>,<span style="color:#a31515">&#34;NLP&#34;</span>,<span style="color:#a31515">&#34;tm&#34;</span>,<span style="color:#a31515">&#34;quanteda&#34;</span>, prompt = T)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cloud_gen &lt;- function(docs){
</span></span><span style="display:flex;"><span>  words.vec &lt;- VectorSource(docs)
</span></span><span style="display:flex;"><span>  words.corpus &lt;- Corpus(words.vec)
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>  tdm &lt;- TermDocumentMatrix(words.corpus)
</span></span><span style="display:flex;"><span>  m &lt;- as.matrix(tdm)
</span></span><span style="display:flex;"><span>  wordCounts &lt;- rowSums(m)
</span></span><span style="display:flex;"><span>  wordCounts &lt;- sort(wordCounts, decreasing=<span style="color:#00f">TRUE</span>)
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>  cloudFrame&lt;-data.frame(word=names(wordCounts),freq=wordCounts)
</span></span><span style="display:flex;"><span>  set.seed(1234)
</span></span><span style="display:flex;"><span>  wordcloud(names(wordCounts),wordCounts, min.freq=3,random.order=<span style="color:#00f">FALSE</span>, max.words=500,scale=c(3,.5), rot.per=0.35,colors=brewer.pal(8,<span style="color:#a31515">&#34;Dark2&#34;</span>))
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cloud_gen(blm_hashtags)
</span></span><span style="display:flex;"><span>cloud_gen(pla_hashtags)
</span></span><span style="display:flex;"><span>cloud_gen(covid_hashtags)
</span></span></code></pre></div><p>You can see the results below. From the &ldquo;Black Lives Matter&rdquo; tweets:
<img src="/blm_wc.png" alt="BLM"></p>
<p>From the &ldquo;PLA Taiwan&rdquo; tweets:
<img src="/pla_wc.png" alt=""></p>
<p>From the &ldquo;COVID vaccines&rdquo; tweets:</p>
<p><img src="/covid_wc.png" alt=""></p>
]]></description>
      
    </item>
    
    <item>
      
      <title>A word cloud</title>
      
      
      <link>https://venkiphy6.github.io/writings/01_churchill/</link>
      <guid>https://venkiphy6.github.io/writings/01_churchill/</guid>
      
      
      <pubDate>Tue, 25 Oct 2022 15:39:42 -0500</pubDate>
      
      
      
      <description><![CDATA[<p>I did a short analysis of Winston Churchill&rsquo;s &ldquo;Their finest hour&rdquo;. I did this for a class on data collection and production. Here is a word cloud I created from this analysis:</p>
<p><img src="/Rplot.png" alt="Word cloud"></p>
<p>If you are interested in the code that I used, you can have a look below:</p>
<pre tabindex="0"><code># Data Method: Text mining
# File: textmining1.R
# Theme: Download text data from web and create wordcloud

# Install the easypackages package 
install.packages(&#34;easypackages&#34;)
library(easypackages)

# Load multiple packages using easypackage function &#34;packages&#34;
packages(&#34;XML&#34;,&#34;wordcloud&#34;,&#34;RColorBrewer&#34;,&#34;NLP&#34;,&#34;tm&#34;,&#34;quanteda&#34;, prompt = T)

# Download text data from website
churchLocation &lt;-URLencode(&#34;http://www.historyplace.com/speeches/churchill-hour.htm&#34;)

# use htmlTreeParse function to read and parse paragraphs
doc.html&lt;- htmlTreeParse(churchLocation, useInternal=TRUE)
church &lt;- unlist(xpathApply(doc.html, &#39;//b&#39;, xmlValue))
church &lt;- church[-26] #Getting rid of the unnecessary last document
head(church, 3)

# Vectorize mlk 
words.vec &lt;- VectorSource(church)

# Check the class of words.vec
class(words.vec)

# Create Corpus object for preprocessing
words.corpus &lt;- Corpus(words.vec)
inspect(words.corpus)

# Turn all words to lower case
words.corpus &lt;- tm_map(words.corpus, content_transformer(tolower))

# Remove punctuations, numbers
words.corpus &lt;- tm_map(words.corpus, removePunctuation)
words.corpus &lt;- tm_map(words.corpus, removeNumbers)

# How about stopwords, then uniform bag of words created

words.corpus &lt;- tm_map(words.corpus, removeWords, stopwords(&#34;english&#34;))

# Create Term Document Matrix

tdm &lt;- TermDocumentMatrix(words.corpus)
inspect(tdm)

m &lt;- as.matrix(tdm)
wordCounts &lt;- rowSums(m)
wordCounts &lt;- sort(wordCounts, decreasing=TRUE)
head(wordCounts)

# Create Wordcloud
cloudFrame&lt;-data.frame(word=names(wordCounts),freq=wordCounts)

set.seed(1234)
wordcloud(cloudFrame$word,cloudFrame$freq)
wordcloud(names(wordCounts),wordCounts, min.freq=3,random.order=FALSE, max.words=500,scale=c(3,.5), rot.per=0.35,colors=brewer.pal(8,&#34;Dark2&#34;))


# Run the program on Winston Churchill&#39;s Finest Hour speech?
# http://www.historyplace.com/speeches/churchill-hour.htm
</code></pre>]]></description>
      
    </item>
    
  </channel>
</rss>
